{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"g3fenAnYqNjv"},"source":["# GLOBAL FUNCTIONS AND IMPORTS"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"bRqlAHeVwYAb"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import random\n","import copy\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from sklearn import preprocessing\n","\n","from tqdm.notebook import trange"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"96-kyq2Rt_y7"},"source":["# GAMES"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XaDsxWWKbzue"},"source":["## TIC-TAC-TOE"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"nTY1e6zpbymW"},"outputs":[],"source":["class TicTacToe:\n","    def __init__(self):\n","        self.row_count = 3\n","        self.column_count = 3\n","        self.action_size = self.row_count * self.column_count\n","        \n","    def __repr__(self):\n","        return \"TicTacToe\"\n","        \n","    def get_initial_state(self):\n","        return np.zeros((self.row_count, self.column_count))\n","    \n","    def get_next_state(self, state, action, player):\n","        row = action // self.column_count\n","        column = action % self.column_count\n","        state[row, column] = player\n","        return state\n","    \n","    def get_valid_moves(self, state):\n","        return (np.where(state.reshape(-1) == 0)[0]).astype(np.uint8)\n","    \n","    def check_win(self, state, action):\n","        if action == None:\n","            return False\n","        \n","        row = action // self.column_count\n","        column = action % self.column_count\n","        player = state[row, column]\n","        \n","        return (\n","            np.sum(state[row, :]) == player * self.column_count\n","            or np.sum(state[:, column]) == player * self.row_count\n","            or np.sum(np.diag(state)) == player * self.row_count\n","            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n","        )\n","    \n","    def get_value_and_terminated(self, state, action):\n","        if self.check_win(state, action):\n","            return 1, True\n","        if np.sum(self.get_valid_moves(state)) == 0:\n","            return 0, True\n","        return 0, False\n","    \n","    def get_opponent(self, player):\n","        return -player\n","    \n","    def get_opponent_value(self, value):\n","        return -value\n","    \n","    def change_perspective(self, state, player):\n","        return state * player\n","    \n","    def get_encoded_state(self, state):\n","        encoded_state = np.stack(\n","            (state == -1, state == 0, state == 1)\n","        ).astype(np.float32)\n","        \n","        if len(state.shape) == 3:\n","            encoded_state = np.swapaxes(encoded_state, 0, 1)\n","        \n","        return encoded_state"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"s7gzDyvJnHGa"},"source":["## CONNECT-FOUR"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"vIunKlEbnRc2"},"outputs":[],"source":["class ConnectFour:\n","    def __init__(self):\n","        self.row_count = 6\n","        self.column_count = 7\n","        self.action_size = self.column_count\n","        self.in_a_row = 4\n","        \n","    def __repr__(self):\n","        return \"ConnectFour\"\n","        \n","    def get_initial_state(self):\n","        return np.zeros((self.row_count, self.column_count))\n","    \n","    def get_next_state(self, state, action, player):\n","        row = np.max(np.where(state[:, action] == 0))\n","        state[row, action] = player\n","        return state\n","    \n","    def get_valid_moves(self, state):\n","        return (state[0] == 0).astype(np.uint8)\n","    \n","    def check_win(self, state, action):\n","        if action == None:\n","            return False\n","        \n","        row = np.min(np.where(state[:, action] != 0))\n","        column = action\n","        player = state[row][column]\n","\n","        def count(offset_row, offset_column):\n","            for i in range(1, self.in_a_row):\n","                r = row + offset_row * i\n","                c = action + offset_column * i\n","                if (\n","                    r < 0 \n","                    or r >= self.row_count\n","                    or c < 0 \n","                    or c >= self.column_count\n","                    or state[r][c] != player\n","                ):\n","                    return i - 1\n","            return self.in_a_row - 1\n","\n","        return (\n","            count(1, 0) >= self.in_a_row - 1\n","            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1\n","            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1\n","            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1\n","        )\n","    \n","    def get_value_and_terminated(self, state, action):\n","        if self.check_win(state, action):\n","            return 1, True\n","        if np.sum(self.get_valid_moves(state)) == 0:\n","            return 0, True\n","        return 0, False\n","    \n","    def get_opponent(self, player):\n","        return -player\n","    \n","    def get_opponent_value(self, value):\n","        return -value\n","    \n","    def change_perspective(self, state, player):\n","        return state * player\n","    \n","    def get_encoded_state(self, state):\n","        encoded_state = np.stack(\n","            (state == -1, state == 0, state == 1)\n","        ).astype(np.float32)\n","        \n","        if len(state.shape) == 3:\n","            encoded_state = np.swapaxes(encoded_state, 0, 1)\n","        \n","        return encoded_state"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ji9bNpVAu7Cn"},"source":["## MIGRATIONS"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ptsipYL0uJWS"},"source":["# PLAYERS"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"c-2sqzVQkpzm"},"source":["## ALPHA-ZERO"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"9gMne4LUwS0d"},"outputs":[],"source":["class ResNet(nn.Module):\n","    def __init__(self, game, num_resBlocks, num_hidden, device):\n","        super().__init__()\n","        \n","        self.device = device\n","        self.startBlock = nn.Sequential(\n","            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(num_hidden),\n","            nn.ReLU()\n","        )\n","        \n","        self.backBone = nn.ModuleList(\n","            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n","        )\n","        \n","        self.policyHead = nn.Sequential(\n","            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n","        )\n","        \n","        self.valueHead = nn.Sequential(\n","            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(3),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3 * game.row_count * game.column_count, 1),\n","            nn.Tanh()\n","        )\n","        \n","        self.to(device)\n","        \n","    def forward(self, x):\n","        x = self.startBlock(x)\n","        for resBlock in self.backBone:\n","            x = resBlock(x)\n","        policy = self.policyHead(x)\n","        value = self.valueHead(x)\n","        return policy, value\n","        \n","        \n","class ResBlock(nn.Module):\n","    def __init__(self, num_hidden):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(num_hidden)\n","        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(num_hidden)\n","        \n","    def forward(self, x):\n","        residual = x\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = self.bn2(self.conv2(x))\n","        x += residual\n","        x = F.relu(x)\n","        return x"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"_cfUWk03ClqX"},"outputs":[],"source":["class Node:\n","    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n","        self.game = game\n","        self.args = args\n","        self.state = state\n","        self.parent = parent\n","        self.action_taken = action_taken\n","        self.prior = prior\n","        \n","        self.children = []\n","        \n","        self.visit_count = visit_count\n","        self.value_sum = 0\n","        \n","    def is_fully_expanded(self):\n","        return len(self.children) > 0\n","    \n","    def select(self):\n","        best_child = None\n","        best_ucb = -np.inf\n","        \n","        for child in self.children:\n","            ucb = self.get_ucb(child)\n","            if ucb > best_ucb:\n","                best_child = child\n","                best_ucb = ucb\n","                \n","        return best_child\n","    \n","    def get_ucb(self, child):\n","        if child.visit_count == 0:\n","            q_value = 0\n","        else:\n","            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n","        return q_value + self.args['C'] * (np.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n","    \n","    def expand(self, policy):\n","        for action, prob in enumerate(policy):\n","            if prob > 0:\n","                child_state = self.state.copy()\n","                child_state = self.game.get_next_state(child_state, action, 1)\n","                child_state = self.game.change_perspective(child_state, player=-1)\n","\n","                child = Node(self.game, self.args, child_state, self, action, prob)\n","                self.children.append(child)\n","                \n","        return child\n","            \n","    def backpropagate(self, value):\n","        self.value_sum += value\n","        self.visit_count += 1\n","        \n","        value = self.game.get_opponent_value(value)\n","        if self.parent is not None:\n","            self.parent.backpropagate(value)  \n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"bSjXH_9VyfrY"},"outputs":[],"source":["class MCTS:\n","    def __init__(self, game, args, model):\n","        self.game = game\n","        self.args = args\n","        self.model = model\n","        \n","    @torch.no_grad()\n","    def search(self, state):\n","        root = Node(self.game, self.args, state, visit_count=1)\n","        \n","        policy, _ = self.model(\n","            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n","        )\n","        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n","        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n","            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n","        \n","        valid_moves = self.game.get_valid_moves(state)\n","        policy *= valid_moves\n","        policy /= np.sum(policy)\n","        root.expand(policy)\n","        \n","        for search in range(self.args['num_searches']):\n","            node = root\n","            \n","            while node.is_fully_expanded():\n","                node = node.select()\n","                \n","            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n","            value = self.game.get_opponent_value(value)\n","            \n","            if not is_terminal:\n","                policy, value = self.model(\n","                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n","                )\n","                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n","                valid_moves = self.game.get_valid_moves(node.state)\n","                policy *= valid_moves\n","                policy /= np.sum(policy)\n","                \n","                value = value.item()\n","                \n","                node.expand(policy)\n","                \n","            node.backpropagate(value)\n","            \n","        action_probs = np.zeros(self.game.action_size)\n","        for child in root.children:\n","            action_probs[child.action_taken] = child.visit_count\n","        action_probs /= np.sum(action_probs)\n","        return action_probs"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"dMocSb7LEGqn"},"outputs":[],"source":["class AlphaZero:\n","    def __init__(self, model, optimizer, game, args):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.game = game\n","        self.args = args\n","        self.mcts = MCTS(game, args, model)\n","        \n","    def selfPlay(self):\n","        memory = []\n","        player = 1\n","        state = self.game.get_initial_state()\n","        \n","        while True:\n","            neutral_state = self.game.change_perspective(state, player)\n","            action_probs = self.mcts.search(neutral_state)\n","            \n","            memory.append((neutral_state, action_probs, player))\n","            \n","            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n","            temperature_action_probs /= np.sum(temperature_action_probs)\n","            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n","            \n","            state = self.game.get_next_state(state, action, player)\n","            \n","            value, is_terminal = self.game.get_value_and_terminated(state, action)\n","            \n","            if is_terminal:\n","                returnMemory = []\n","                for hist_neutral_state, hist_action_probs, hist_player in memory:\n","                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n","                    returnMemory.append((\n","                        self.game.get_encoded_state(hist_neutral_state),\n","                        hist_action_probs,\n","                        hist_outcome\n","                    ))\n","                return returnMemory\n","            \n","            player = self.game.get_opponent(player)\n","                \n","    def train(self, memory):\n","        random.shuffle(memory)\n","        for batchIdx in range(0, len(memory), self.args['batch_size']):\n","            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n","            state, policy_targets, value_targets = zip(*sample)\n","            \n","            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n","            \n","            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n","            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n","            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n","            \n","            out_policy, out_value = self.model(state)\n","            \n","            policy_loss = F.cross_entropy(out_policy, policy_targets)\n","            value_loss = F.mse_loss(out_value, value_targets)\n","            loss = policy_loss + value_loss\n","            \n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","    \n","    def learn(self):\n","        for iteration in range(self.args['num_iterations']):\n","            memory = []\n","            \n","            self.model.eval()\n","            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n","                memory += self.selfPlay()\n","                \n","            self.model.train()\n","            for epoch in trange(self.args['num_epochs']):\n","                self.train(memory)\n","            \n","            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n","            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"]},{"cell_type":"markdown","metadata":{"id":"rZXFePY_yIP5"},"source":["##ALPHA-ZERO-PARALLEL"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"C1rwf7NIyOZi"},"outputs":[],"source":["class MCTSParallel:\n","    def __init__(self, game, args, model):\n","        self.game = game\n","        self.args = args\n","        self.model = model\n","        \n","    @torch.no_grad()\n","    def search(self, states, spGames):\n","        policy, _ = self.model(\n","            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n","        )\n","        policy = torch.softmax(policy, axis=1).cpu().numpy()\n","        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n","            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n","        \n","        for i, spg in enumerate(spGames):\n","            spg_policy = policy[i]\n","            valid_moves = self.game.get_valid_moves(states[i])\n","            spg_policy *= valid_moves\n","            spg_policy /= np.sum(spg_policy)\n","\n","            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n","            spg.root.expand(spg_policy)\n","        \n","        for search in range(self.args['num_searches']):\n","            for spg in spGames:\n","                spg.node = None\n","                node = spg.root\n","\n","                while node.is_fully_expanded():\n","                    node = node.select()\n","\n","                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n","                value = self.game.get_opponent_value(value)\n","                \n","                if is_terminal:\n","                    node.backpropagate(value)\n","                    \n","                else:\n","                    spg.node = node\n","                    \n","            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n","                    \n","            if len(expandable_spGames) > 0:\n","                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n","                \n","                policy, value = self.model(\n","                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n","                )\n","                policy = torch.softmax(policy, axis=1).cpu().numpy()\n","                value = value.cpu().numpy()\n","                \n","            for i, mappingIdx in enumerate(expandable_spGames):\n","                node = spGames[mappingIdx].node\n","                spg_policy, spg_value = policy[i], value[i]\n","                \n","                valid_moves = self.game.get_valid_moves(node.state)\n","                spg_policy *= valid_moves\n","                spg_policy /= np.sum(spg_policy)\n","\n","                node.expand(spg_policy)\n","                node.backpropagate(spg_value)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"vIeYMY9GxvU4"},"outputs":[],"source":["class AlphaZeroParallel:\n","    def __init__(self, model, optimizer, game, args):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.game = game\n","        self.args = args\n","        self.mcts = MCTSParallel(game, args, model)\n","        \n","    def selfPlay(self):\n","        return_memory = []\n","        player = 1\n","        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n","        \n","        while len(spGames) > 0:\n","            states = np.stack([spg.state for spg in spGames])\n","            neutral_states = self.game.change_perspective(states, player)\n","            \n","            self.mcts.search(neutral_states, spGames)\n","            \n","            for i in range(len(spGames))[::-1]:\n","                spg = spGames[i]\n","                \n","                action_probs = np.zeros(self.game.action_size)\n","                for child in spg.root.children:\n","                    action_probs[child.action_taken] = child.visit_count\n","                action_probs /= np.sum(action_probs)\n","\n","                spg.memory.append((spg.root.state, action_probs, player))\n","\n","                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n","                temperature_action_probs /= np.sum(temperature_action_probs)\n","                action = np.random.choice(self.game.action_size, p=temperature_action_probs) # Divide temperature_action_probs with its sum in case of an error\n","\n","                spg.state = self.game.get_next_state(spg.state, action, player)\n","\n","                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n","\n","                if is_terminal:\n","                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n","                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n","                        return_memory.append((\n","                            self.game.get_encoded_state(hist_neutral_state),\n","                            hist_action_probs,\n","                            hist_outcome\n","                        ))\n","                    del spGames[i]\n","                    \n","            player = self.game.get_opponent(player)\n","            \n","        return return_memory\n","                \n","    def train(self, memory):\n","        random.shuffle(memory)\n","        for batchIdx in range(0, len(memory), self.args['batch_size']):\n","            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n","            state, policy_targets, value_targets = zip(*sample)\n","            \n","            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n","            \n","            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n","            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n","            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n","            \n","            out_policy, out_value = self.model(state)\n","            \n","            policy_loss = F.cross_entropy(out_policy, policy_targets)\n","            value_loss = F.mse_loss(out_value, value_targets)\n","            loss = policy_loss + value_loss\n","            \n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","    \n","    def learn(self):\n","        for iteration in range(self.args['num_iterations']):\n","            memory = []\n","            \n","            self.model.eval()\n","            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n","                memory += self.selfPlay()\n","            \n","            self.model.train()\n","            for epoch in range(self.args['num_epochs']):\n","                self.train(memory)\n","            \n","            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n","            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"QPRRTJhMyuJz"},"outputs":[],"source":["class SPG:\n","    def __init__(self, game):\n","        self.state = game.get_initial_state()\n","        self.memory = []\n","        self.root = None\n","        self.node = None\n","        "]},{"cell_type":"markdown","metadata":{"id":"WZIFIgBZsV_8"},"source":["##EVALUATE-AI"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"JEPt8M7tsVVC"},"outputs":[],"source":["class BigBrain(nn.Module):\n","    def __init__(self, game, num_resBlocks, num_hidden, device):\n","        super().__init__()\n","        \n","        self.device = device\n","        self.startBlock = nn.Sequential(\n","            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(num_hidden),\n","            nn.ReLU()\n","        )\n","        \n","        self.backBone = nn.ModuleList(\n","            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n","        )\n","        \n","        self.valueHead = nn.Sequential(\n","            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(3),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3 * game.row_count * game.column_count, 1),\n","            nn.Tanh()\n","        )\n","        \n","        self.to(device)\n","        \n","    def forward(self, x):\n","        x = self.startBlock(x)\n","        for resBlock in self.backBone:\n","            x = resBlock(x)\n","        value = self.valueHead(x)\n","        return value\n","        \n","class ResBlock(nn.Module):\n","    def __init__(self, num_hidden):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(num_hidden)\n","        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(num_hidden)\n","        \n","    def forward(self, x):\n","        residual = x\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = self.bn2(self.conv2(x))\n","        x += residual\n","        x = F.relu(x)\n","        return x"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"ef7YiylitVP7"},"outputs":[],"source":["class evaluateAI:\n","    def __init__(self, model, optimizer, game, args):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.game = game\n","        self.args = args\n","        \n","    def selfPlay(self):\n","        memory = []\n","        player = 1\n","        state = self.game.get_initial_state()\n","        \n","        while True:\n","            neutral_state = self.game.change_perspective(state, player)\n","\n","            action_probs = dict()\n","            for action in self.game.get_valid_moves(state):\n","                state_after_action = self.game.get_next_state(np.copy(state), action, player)\n","                new_neutral_state = self.game.change_perspective(state_after_action, self.game.get_opponent(player))\n","\n","                value = self.model(\n","                    torch.tensor(self.game.get_encoded_state(new_neutral_state), device=self.model.device).unsqueeze(0)\n","                )\n","                action_probs[action] = self.game.get_opponent_value(value) + 1\n","            \n","            memory.append((neutral_state, player))\n","            action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n","            state = self.game.get_next_state(state, action, player)\n","            value, is_terminal = self.game.get_value_and_terminated(state, action)\n","            \n","            if is_terminal:\n","                returnMemory = []\n","                for hist_neutral_state, hist_player in memory:\n","                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n","                    returnMemory.append((\n","                        self.game.get_encoded_state(hist_neutral_state),\n","                        hist_outcome\n","                    ))\n","                return returnMemory\n","            player = self.game.get_opponent(player)\n","                \n","    def train(self, memory):\n","        random.shuffle(memory)\n","        for batchIdx in range(0, len(memory), self.args['batch_size']):\n","            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n","            state, value_targets = zip(*sample)\n","            \n","            state, value_targets = np.array(state), np.array(value_targets).reshape(-1, 1)\n","            \n","            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n","            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n","            \n","            out_value = self.model(state)\n","            \n","            value_loss = F.mse_loss(out_value, value_targets)\n","            loss = value_loss\n","            \n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","    \n","    def learn(self):\n","        for iteration in range(self.args['num_iterations']):\n","            memory = []\n","            \n","            self.model.eval()\n","            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n","                memory += self.selfPlay()\n","                \n","            self.model.train()\n","            for epoch in range(self.args['num_epochs']):\n","                self.train(memory)\n","            \n","            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n","            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"]},{"cell_type":"markdown","metadata":{"id":"wwFOCbM18f-2"},"source":["##EVALUATE-AI-PARALLEL"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"zipWWWZQ8jag"},"outputs":[],"source":["class evaluateAIParallel:\n","    def __init__(self, model, optimizer, game, args):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.game = game\n","        self.args = args\n","        \n","    def selfPlay(self):\n","        return_memory = []\n","        player = 1\n","        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n","        \n","        while len(spGames) > 0:\n","            states = np.stack([spg.state for spg in spGames])\n","            neutral_states = self.game.change_perspective(states, player)\n","            \n","            for i in range(len(spGames))[::-1]:\n","                spg = spGames[i]\n","                \n","                action_probs = dict()\n","                for action in self.game.get_valid_moves(states[i]):\n","                    state_after_action = self.game.get_next_state(np.copy(states[i]), action, player)\n","                    new_neutral_state = self.game.change_perspective(state_after_action, self.game.get_opponent(player))\n","\n","                    value = self.model(\n","                        torch.tensor(self.game.get_encoded_state(new_neutral_state), device=self.model.device).unsqueeze(0)\n","                    )\n","                    action_probs[action] = self.game.get_opponent_value(value.item()) / 2 + 0.5\n","                \n","                action_probs.update((key, value**self.args['temperature']) for key, value in action_probs.items())\n","                spg.memory.append((neutral_states[i], player))\n","                action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n","                spg.state = self.game.get_next_state(spg.state, action, player)\n","                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n","                if is_terminal:\n","                    for hist_neutral_state, hist_player in spg.memory:\n","                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n","                        return_memory.append((\n","                            self.game.get_encoded_state(hist_neutral_state),\n","                            hist_outcome\n","                        ))\n","                    del spGames[i]\n","                    \n","            player = self.game.get_opponent(player)\n","            \n","        return return_memory\n","                \n","    def train(self, memory):\n","        random.shuffle(memory)\n","        for batchIdx in range(0, len(memory), self.args['batch_size']):\n","            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n","            state, value_targets = zip(*sample)\n","            \n","            state, value_targets = np.array(state), np.array(value_targets).reshape(-1, 1)\n","            \n","            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n","            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n","            \n","            out_value = self.model(state)\n","            \n","            value_loss = F.mse_loss(out_value, value_targets)\n","            loss = value_loss\n","            \n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","    \n","    def learn(self):\n","        for iteration in range(self.args['num_iterations']):\n","            memory = []\n","            \n","            self.model.eval()\n","            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n","                new_memories = self.selfPlay()\n","                memory += new_memories\n","            \n","            self.model.train()\n","            for epoch in range(self.args['num_epochs']):\n","                self.train(memory)\n","            \n","            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n","            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n"]},{"cell_type":"markdown","metadata":{"id":"ll-vbL9tuN7e"},"source":["##DUMMY"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"SeGQH2FmcPaX"},"outputs":[],"source":["class dummy():\n","    def choose_action(self, game, game_state):\n","        return random.choice(game.actions(game_state))"]},{"cell_type":"markdown","metadata":{"id":"FG7virNNvBrP"},"source":["##ALPHA-BETA"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"1CCW7ugWvJGa"},"outputs":[],"source":["class alphabeta():\n","    def choose_action(self, game, game_state):\n","        player = game_state.player\n","        value, move = self.max_value(game, game_state, player, -np.Inf, np.Inf)\n","        return move\n","\n","    def max_value(self, game, game_state, player, alfa, beta):\n","        if game.is_terminal(game_state):\n","            return game.utility(player, game_state), None\n","        value = -np.Inf\n","\n","        for action in game.actions(game_state):\n","            value2, action2 = self.min_value(game, game.perform_action(action, game_state), player, alfa, beta)\n","            if value2 > value:\n","                value, move = value2, action\n","                alfa = max(alfa, value)\n","            if value >= beta:\n","                return value, move\n","        return value, move\n","    \n","    def min_value(self, game, game_state, player, alfa, beta):\n","        if game.is_terminal(game_state):\n","            return game.utility(player, game_state), None\n","        value = np.Inf\n","\n","        for action in game.actions(game_state):\n","            value2, action2 = self.max_value(game, game.perform_action(action, game_state), player, alfa, beta)\n","            if value2 < value:\n","                value, move = value2, action\n","                beta = min(beta, value)\n","            if value <= alfa:\n","                return value, move\n","        return value, move"]},{"cell_type":"markdown","metadata":{"id":"EL64pm3PkkiD"},"source":["##MCTS"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"EtllgetE5yGF"},"outputs":[],"source":["class MCTSNode:\n","    def __init__(self, game, args, state, parent = None, action_taken = None):\n","        self.game = game\n","        self.args = args\n","        self.state = state\n","        self.parent = parent\n","        self.action_taken = action_taken\n","\n","        self.children = []\n","        self.expandable_moves = game.get_valid_moves(state)\n","\n","        self.visit_count = 0\n","        self.value_sum = 0\n","\n","    def is_fully_expanded(self):\n","        return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n","    \n","    def get_ucb(self, child):\n","        q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n","        return q_value + self.args['c'] * np.sqrt(np.log(self.visit_count) / child.visit_count)\n","    \n","    def select(self):\n","        best_child = None\n","        best_ucb = -np.inf\n","        for child in self.children:\n","            ucb = self.get_ucb(child)\n","            if ucb > best_ucb:\n","                best_child = child\n","                best_ucb = ucb\n","        return best_child\n","    \n","    def expand(self):\n","        action = np.random.choice(np.where(self.expandable_moves == 1)[0])\n","        self.expandable_moves[action] = 0\n","        child_state = self.state.copy()\n","        child_state = self.game.get_next_state(child_state, action, 1)\n","        child_state = self.game.change_perspective(child_state, player = -1)\n","\n","        child = MCTSNode(self.game, self.args, child_state, self, action)\n","        self.children.append(child)\n","        return child\n","\n","    def simulate(self):\n","        value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n","        value = self.game.get_opponent_value(value)\n","\n","        if is_terminal:\n","            return value\n","        \n","        rollout_state = self.state.copy()\n","        rollout_player = 1\n","        while True:\n","            valid_moves = self.game.get_valid_moves(rollout_state)\n","            action = np.random.choice(np.where(valid_moves == 1)[0])\n","            rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n","            value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n","\n","            if is_terminal:\n","                if rollout_player == -1:\n","                    value = self.game.get_opponent_value(value)\n","                return value\n","            rollout_player = self.game.get_opponent(rollout_player)\n","    \n","    def backpropagate(self, value):\n","        self.value_sum += value\n","        self.visit_count += 1\n","        value = self.game.get_opponent_value(value)\n","        if self.parent is not None:\n","            self.parent.backpropagate(value)\n","\n","class MCTS:\n","    def __init__(self, game, args, model):\n","        self.game = game\n","        self.args = args\n","    \n","    def search(self, state):\n","        root = MCTSNode(self.game, self.args, state)\n","\n","        for serach in range(self.args['num_searches']):\n","            node = root\n","\n","            while node.is_fully_expanded():\n","                node = node.select()\n","            \n","            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n","            value = self.game.get_opponent_value(value)\n","\n","            if not is_terminal:\n","                node = node.expand()\n","                value = node.simulate()\n","            node.backpropagate(value)\n","        \n","        action_probabilities = np.zeros(self.game.action_size)\n","        for child in root.children:\n","            action_probabilities[child.action_taken] = child.visit_count\n","        action_probabilities /= np.sum(action_probabilities)\n","        return action_probabilities"]},{"cell_type":"markdown","metadata":{"id":"qL_I0vnEk1Rx"},"source":["#TRAINING"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"pBkdJFzhN7fN"},"outputs":[],"source":["class ELO_Tournament_Trainer:\n","    def __init__(self, game, device, args, model, optimizer, number_of_players = 4, players = []):\n","        self.game = game\n","        self.number_of_players = number_of_players\n","        self.players = players\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.args = args\n","        self.model = model\n","        self.optimizer = optimizer\n","    \n","    def train_models(self, no_improvement = 10):\n","        while len(self.players) < self.number_of_players:\n","            player = evaluateAIParallel(copy.deepcopy(self.model), copy.deepcopy(self.optimizer), game, self.args)\n","            player.learn()\n","            self.players.append(copy.deepcopy(player))\n","        \n","        results = self.tournament()\n","        \n","        best_performer = max(results, key=results.get)\n","        worst_performer = min(results, key=results.get)\n","        print(f'best_performence: {max(results.values())}')\n","        print(f'worst_performence: {min(results.values())}')\n","        return self.players\n","    \n","    def tournament(self):\n","        players_performances = {player: 0 for player in self.players}\n","        for player1 in self.players:\n","            for player2 in self.players:\n","                for _ in range(10):\n","                    result = self.play_game(player1, player2)\n","                    if result == 1:\n","                        players_performances[player2] -= 1\n","                    if result == -1:\n","                        players_performances[player1] -= 1\n","        return players_performances\n","\n","    def play_game(self, player1, player2, debug = False):\n","        state = self.game.get_initial_state()\n","        player = 1\n","        while True:\n","            if debug:\n","                print(state)\n","            action = self.choose_action(state, player, player1.model) if player == 1 else self.choose_action(state, player, player2.model)\n","            state = self.game.get_next_state(state, action, player)  \n","            value, is_terminal = self.game.get_value_and_terminated(state, action)\n","\n","            if is_terminal:\n","                if value == 1:\n","                    return player\n","                else:\n","                    return 0\n","            player = self.game.get_opponent(player)\n","    \n","    def choose_action(self, state, player, model):\n","        neutral_state = self.game.change_perspective(state, player)\n","\n","        action_probs = dict()\n","        for action in self.game.get_valid_moves(state):\n","            state_after_action = self.game.get_next_state(np.copy(state), action, player)\n","            new_neutral_state = self.game.change_perspective(state_after_action, self.game.get_opponent(player))\n","\n","            value = model(\n","                torch.tensor(self.game.get_encoded_state(new_neutral_state), device=model.device).unsqueeze(0)\n","            )\n","            value = value.item()\n","            action_probs[action] = self.game.get_opponent_value(value) / 2 + 0.5\n","        \n","        if self.args['temperature'] == 'inf':\n","            return max(action_probs, key = action_probs.get)\n","        \n","        action_probs.update((key, value**self.args['temperature']) for key, value in action_probs.items())\n","        return random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]"]},{"cell_type":"markdown","metadata":{"id":"oewiz67-kKaA"},"source":["##TIC-TAC-TOE-TRAINING"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377,"referenced_widgets":["2cf69e08b4f344dc9c4caf8cedfcf0b5","3d70243d69ad4b28856e3415766eda96","5659a3ef2b604d10b3b48c5c10142463","2581ea1a497143cb9647052f33e78282","67ba1450ede0436b9dc0991131a7b5b4","16f49f485d934986a591968749876747","66f4a4e4e4ab4085951937c9272ba824","1379faa97599448c94c7767e22ff8829","5235559ca2e54479a8e97fa7f4e5bebf","7eddf86b08fb404781fdfc1ae918544c","03f52c5bbabc45beb0f69b3bb2648544"]},"executionInfo":{"elapsed":6778,"status":"error","timestamp":1679411131161,"user":{"displayName":"Eryk","userId":"06461886407302871561"},"user_tz":-60},"id":"srtg_3Sd1KEr","outputId":"f40d2124-3e1a-4832-812a-53f911837c6d"},"outputs":[{"ename":"ImportError","evalue":"IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[1;32mIn[36], line 16\u001b[0m\n\u001b[0;32m      5\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_iterations\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_selfPlay_iterations\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m512\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdirichlet_alpha\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.3\u001b[39m\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     15\u001b[0m trainer \u001b[39m=\u001b[39m ELO_Tournament_Trainer(game, device, args, model, optimizer)\n\u001b[1;32m---> 16\u001b[0m players \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain_models()\n","Cell \u001b[1;32mIn[35], line 14\u001b[0m, in \u001b[0;36mELO_Tournament_Trainer.train_models\u001b[1;34m(self, no_improvement)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayers) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumber_of_players:\n\u001b[0;32m     13\u001b[0m     player \u001b[39m=\u001b[39m evaluateAIParallel(copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel), copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer), game, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs)\n\u001b[1;32m---> 14\u001b[0m     player\u001b[39m.\u001b[39;49mlearn()\n\u001b[0;32m     15\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayers\u001b[39m.\u001b[39mappend(copy\u001b[39m.\u001b[39mdeepcopy(player))\n\u001b[0;32m     17\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtournament()\n","Cell \u001b[1;32mIn[31], line 73\u001b[0m, in \u001b[0;36mevaluateAIParallel.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m memory \u001b[39m=\u001b[39m []\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> 73\u001b[0m \u001b[39mfor\u001b[39;00m selfPlay_iteration \u001b[39min\u001b[39;00m trange(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs[\u001b[39m'\u001b[39;49m\u001b[39mnum_selfPlay_iterations\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs[\u001b[39m'\u001b[39;49m\u001b[39mnum_parallel_games\u001b[39;49m\u001b[39m'\u001b[39;49m]):\n\u001b[0;32m     74\u001b[0m     new_memories \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselfPlay()\n\u001b[0;32m     75\u001b[0m     memory \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_memories\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\notebook.py:316\u001b[0m, in \u001b[0;36mtnrange\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtnrange\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    315\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Shortcut for `tqdm.notebook.tqdm(range(*args), **kwargs)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[39mreturn\u001b[39;00m tqdm_notebook(\u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39margs), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\notebook.py:238\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m    237\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[1;32m--> 238\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[0;32m    239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\notebook.py:113\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[0;32m    115\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n","\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"]}],"source":["game = TicTacToe()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = BigBrain(game, 8, 8, device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n","args = {\n","    'num_iterations': 1,\n","    'num_selfPlay_iterations': 512,\n","    'num_parallel_games': 128,\n","    'num_epochs': 4,\n","    'batch_size': 128,\n","    'temperature': 1,\n","    'dirichlet_epsilon': 0.25,\n","    'dirichlet_alpha': 0.3\n","}\n","trainer = ELO_Tournament_Trainer(game, device, args, model, optimizer)\n","players = trainer.train_models()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iItn3Yg626mk"},"outputs":[],"source":["players"]},{"cell_type":"markdown","metadata":{"id":"1BbNpeNbkRJL"},"source":["##CONNECT-FOUR-TRAINING"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6Z3Kq3KkU0k"},"outputs":[],"source":["game = ConnectFour()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = BigBrain(game, 8, 8, device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n","args = {\n","    'num_iterations': 1,\n","    'num_selfPlay_iterations': 512,\n","    'num_parallel_games': 128,\n","    'num_epochs': 4,\n","    'batch_size': 128,\n","    'temperature': 1,\n","    'dirichlet_epsilon': 0.25,\n","    'dirichlet_alpha': 0.3\n","}\n","trainer = ELO_Tournament_Trainer(game, device, args, model, optimizer)\n","players = trainer.train_models()"]},{"cell_type":"markdown","metadata":{"id":"DZPgFsjiJ1EQ"},"source":["#TESTS"]},{"cell_type":"markdown","metadata":{"id":"0La_wyg1qApl"},"source":["##TIC-TAC-TOE-TESTS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRG6Qtv0J4_U"},"outputs":[],"source":["game = TicTacToe()\n","win1 = np.array([[0, 0, 1], [-1, 1, 0], [1, -1, 0]])\n","win2 = np.array([[1, 0, -1], [1, -1, 0], [1, 0, 0]])\n","win3 = np.array([[0, 0, 0], [0, -1, -1], [1, 1, 1]])\n","win4 = np.array([[1, 0, -1], [1, -1, 0], [0, 0, 0]])\n","win5 = np.array([[0, 0, 1], [-1, 0, 0], [1, -1, 0]])\n","wins = [win1, win2, win3, win4, win5]\n","\n","draw1 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n","draw2 = np.array([[0, 0, 0], [0, -1, 0], [0, 0, 0]])\n","draw3 = np.array([[-1, 1, -1], [-1, 1, -1], [1, -1, 1]])\n","draws = [draw1, draw2, draw3]\n","\n","lose1 = np.array([[0, 0, -1], [1, -1, 0], [-1, 1, 0]])\n","lose2 = np.array([[-1, 0, -1], [1, 0, 0], [-1, 1, 0]])\n","lose3 = np.array([[-1, 0, 1], [-1, 0, 0], [-1, 1, 0]])\n","loses = [lose1, lose2, lose3]\n","\n","for i, player in enumerate(players):\n","    print(f'model{i}')\n","    model = players[i].model\n","    model.eval()\n","    print('winning_possitions:')\n","    for winning_position in wins:\n","        encoded_state = game.get_encoded_state(winning_position)\n","        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n","\n","        value = model(tensor_state)\n","        value = value.item()\n","        print(value)\n","\n","    print('\\ndrawing_possitions:')\n","    for drawing_position in draws:\n","        encoded_state = game.get_encoded_state(drawing_position)\n","        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n","\n","        value = model(tensor_state)\n","        value = value.item()\n","        print(value)\n","\n","    print('\\nlosing_possitions:')\n","    for losing_position in loses:\n","        encoded_state = game.get_encoded_state(losing_position)\n","        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n","\n","        value = model(tensor_state)\n","        value = value.item()\n","        print(value)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"LyhyUevTtFbh"},"source":["#TOURNAMENTS"]},{"cell_type":"markdown","metadata":{"id":"0FVITMsKtLEj"},"source":["##TIC-TAC-TOE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OonkC4p0p-xW"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"jt6MmQY76_1x"},"source":["#ADDITIONAL RESOURCES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztXYQ-NW7Brw"},"outputs":[],"source":["class CRAZY_FAST_LEARNING:\n","    def __init__(self, game, device, args, number_of_players = 4, players = []):\n","        self.game = game\n","        self.number_of_players = number_of_players\n","        self.players = players\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.args = args\n","    \n","    def train_models(self, no_improvement = 10):\n","        while len(self.players) < self.number_of_players:\n","            model = BigBrain(game, 8, 8, device)\n","            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n","            player = evaluateAIParallel(model, optimizer, game, self.args)\n","            player.learn()\n","            self.players.append(copy.deepcopy(player))\n","        \n","        while no_improvement > 9:\n","            results = self.tournament()\n","            \n","            best_performer = max(results, key=results.get)\n","            worst_performer = min(results, key=results.get)\n","            print(f'best_performence: {max(results.values())}')\n","            print(f'worst_performence: {min(results.values())}')\n","            self.players.remove(worst_performer)\n","            best_performer = copy.deepcopy(best_performer)\n","            best_performer.learn()\n","            self.players.append(best_performer)\n","            no_improvement -= 1\n","        return self.players\n","    \n","    def tournament(self):\n","        players_performances = {player: 0 for player in self.players}\n","        for player1 in self.players:\n","            for player2 in self.players:\n","                for _ in range(10):\n","                    result = self.play_game(player1, player2)\n","                    if result == 1:\n","                        players_performances[player2] -= 1\n","                    if result == -1:\n","                        players_performances[player1] -= 1\n","        return players_performances\n","\n","    def play_game(self, player1, player2):\n","        state = self.game.get_initial_state()\n","        player = 1\n","        while True:\n","            action = self.choose_action(state, player, player1.model) if player == 1 else self.choose_action(state, player, player2.model)\n","            state = self.game.get_next_state(state, action, player)  \n","            value, is_terminal = self.game.get_value_and_terminated(state, action)\n","\n","            if is_terminal:\n","                if value == 1:\n","                    return player\n","                else:\n","                    return 0\n","            player = self.game.get_opponent(player)\n","    \n","    def choose_action(self, state, player, model):\n","        neutral_state = self.game.change_perspective(state, player)\n","\n","        action_probs = dict()\n","        for action in self.game.get_valid_moves(state):\n","            state_after_action = self.game.get_next_state(np.copy(state), action, player)\n","            new_neutral_state = self.game.change_perspective(state_after_action, self.game.get_opponent(player))\n","            \n","            value = model(\n","                torch.tensor(self.game.get_encoded_state(new_neutral_state), device=model.device).unsqueeze(0)\n","            )\n","            value = value.item()\n","            action_probs[action] = self.game.get_opponent_value(value) / 2 + 0.5\n","        \n","        if self.args['temperature'] == 'inf':\n","            return max(action_probs, key = action_probs.get)\n","        \n","        action_probs.update((key, value**self.args['temperature']) for key, value in action_probs.items())\n","        return random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iww9T60s7KTt"},"outputs":[],"source":["game = TicTacToe()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","args = {\n","    'num_iterations': 1,\n","    'num_selfPlay_iterations': 1024,\n","    'num_parallel_games': 128,\n","    'num_epochs': 4,\n","    'batch_size': 128,\n","    'temperature': 1,\n","    'dirichlet_epsilon': 0.25,\n","    'dirichlet_alpha': 0.3\n","}\n","trainer = CRAZY_FAST_LEARNING(game, device, args)\n","players = trainer.train_models()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONbuTYjb7M7D"},"outputs":[],"source":["players"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G4ww-ndn7OwY"},"outputs":[],"source":["game = TicTacToe()\n","win1 = np.array([[0, 0, 1], [-1, 1, 0], [1, -1, 0]])\n","win2 = np.array([[1, 0, -1], [1, -1, 0], [1, 0, 0]])\n","win3 = np.array([[0, 0, 0], [0, -1, -1], [1, 1, 1]])\n","win4 = np.array([[1, 0, -1], [1, -1, 0], [0, 0, 0]])\n","win5 = np.array([[0, 0, 1], [-1, 0, 0], [1, -1, 0]])\n","wins = [win1, win2, win3, win4, win5]\n","\n","draw1 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n","draw2 = np.array([[0, 0, 0], [0, -1, 0], [0, 0, 0]])\n","draw3 = np.array([[-1, 1, -1], [-1, 1, -1], [1, -1, 1]])\n","draws = [draw1, draw2, draw3]\n","\n","lose1 = np.array([[0, 0, -1], [1, -1, 0], [-1, 1, 0]])\n","lose2 = np.array([[-1, 0, -1], [1, 0, 0], [-1, 1, 0]])\n","lose3 = np.array([[-1, 0, 1], [-1, 0, 0], [-1, 1, 0]])\n","loses = [lose1, lose2, lose3]\n","\n","for i, player in enumerate(players):\n","    print(f'model{i}')\n","    model = players[1].model\n","    model.eval()\n","    print('winning_possitions:')\n","    for winning_position in wins:\n","        encoded_state = game.get_encoded_state(winning_position)\n","        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n","\n","        value = model(tensor_state)\n","        value = value.item()\n","        print(value)\n","\n","    print('\\ndrawing_possitions:')\n","    for drawing_position in draws:\n","        encoded_state = game.get_encoded_state(drawing_position)\n","        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n","\n","        value = model(tensor_state)\n","        value = value.item()\n","        print(value)\n","\n","    print('\\nlosing_possitions:')\n","    for losing_position in loses:\n","        encoded_state = game.get_encoded_state(losing_position)\n","        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n","\n","        value = model(tensor_state)\n","        value = value.item()\n","        print(value)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNBB97QUomwqZOtf3SaXUfQ","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03f52c5bbabc45beb0f69b3bb2648544":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1379faa97599448c94c7767e22ff8829":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16f49f485d934986a591968749876747":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2581ea1a497143cb9647052f33e78282":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7eddf86b08fb404781fdfc1ae918544c","placeholder":"​","style":"IPY_MODEL_03f52c5bbabc45beb0f69b3bb2648544","value":" 0/4 [00:06&lt;?, ?it/s]"}},"2cf69e08b4f344dc9c4caf8cedfcf0b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d70243d69ad4b28856e3415766eda96","IPY_MODEL_5659a3ef2b604d10b3b48c5c10142463","IPY_MODEL_2581ea1a497143cb9647052f33e78282"],"layout":"IPY_MODEL_67ba1450ede0436b9dc0991131a7b5b4"}},"3d70243d69ad4b28856e3415766eda96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16f49f485d934986a591968749876747","placeholder":"​","style":"IPY_MODEL_66f4a4e4e4ab4085951937c9272ba824","value":"  0%"}},"5235559ca2e54479a8e97fa7f4e5bebf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5659a3ef2b604d10b3b48c5c10142463":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_1379faa97599448c94c7767e22ff8829","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5235559ca2e54479a8e97fa7f4e5bebf","value":0}},"66f4a4e4e4ab4085951937c9272ba824":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67ba1450ede0436b9dc0991131a7b5b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eddf86b08fb404781fdfc1ae918544c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
