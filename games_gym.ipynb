{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3fenAnYqNjv"
      },
      "source": [
        "#GLOBAL FUNCTIONS AND IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRqlAHeVwYAb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from tqdm.notebook import trange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-2sqzVQkpzm"
      },
      "source": [
        "##ALPHA-ZERO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gMne4LUwS0d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cfUWk03ClqX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSjXH_9VyfrY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMocSb7LEGqn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZXFePY_yIP5"
      },
      "source": [
        "##ALPHA-ZERO-PARALLEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1rwf7NIyOZi"
      },
      "outputs": [],
      "source": [
        "class MCTSParallel:\n",
        "    def __init__(self, game, args, model):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.model = model\n",
        "        \n",
        "    @torch.no_grad()\n",
        "    def search(self, states, spGames):\n",
        "        policy, _ = self.model(\n",
        "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
        "        )\n",
        "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
        "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
        "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
        "        \n",
        "        for i, spg in enumerate(spGames):\n",
        "            spg_policy = policy[i]\n",
        "            valid_moves = self.game.get_valid_moves(states[i])\n",
        "            spg_policy *= valid_moves\n",
        "            spg_policy /= np.sum(spg_policy)\n",
        "\n",
        "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
        "            spg.root.expand(spg_policy)\n",
        "        \n",
        "        for search in range(self.args['num_searches']):\n",
        "            for spg in spGames:\n",
        "                spg.node = None\n",
        "                node = spg.root\n",
        "\n",
        "                while node.is_fully_expanded():\n",
        "                    node = node.select()\n",
        "\n",
        "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "                value = self.game.get_opponent_value(value)\n",
        "                \n",
        "                if is_terminal:\n",
        "                    node.backpropagate(value)\n",
        "                    \n",
        "                else:\n",
        "                    spg.node = node\n",
        "                    \n",
        "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
        "                    \n",
        "            if len(expandable_spGames) > 0:\n",
        "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
        "                \n",
        "                policy, value = self.model(\n",
        "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
        "                )\n",
        "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
        "                value = value.cpu().numpy()\n",
        "                \n",
        "            for i, mappingIdx in enumerate(expandable_spGames):\n",
        "                node = spGames[mappingIdx].node\n",
        "                spg_policy, spg_value = policy[i], value[i]\n",
        "                \n",
        "                valid_moves = self.game.get_valid_moves(node.state)\n",
        "                spg_policy *= valid_moves\n",
        "                spg_policy /= np.sum(spg_policy)\n",
        "\n",
        "                node.expand(spg_policy)\n",
        "                node.backpropagate(spg_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIeYMY9GxvU4"
      },
      "outputs": [],
      "source": [
        "class AlphaZeroParallel:\n",
        "    def __init__(self, model, optimizer, game, args):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.mcts = MCTSParallel(game, args, model)\n",
        "        \n",
        "    def selfPlay(self):\n",
        "        return_memory = []\n",
        "        player = 1\n",
        "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
        "        \n",
        "        while len(spGames) > 0:\n",
        "            states = np.stack([spg.state for spg in spGames])\n",
        "            neutral_states = self.game.change_perspective(states, player)\n",
        "            \n",
        "            self.mcts.search(neutral_states, spGames)\n",
        "            \n",
        "            for i in range(len(spGames))[::-1]:\n",
        "                spg = spGames[i]\n",
        "                \n",
        "                action_probs = np.zeros(self.game.action_size)\n",
        "                for child in spg.root.children:\n",
        "                    action_probs[child.action_taken] = child.visit_count\n",
        "                action_probs /= np.sum(action_probs)\n",
        "\n",
        "                spg.memory.append((spg.root.state, action_probs, player))\n",
        "\n",
        "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
        "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
        "                action = np.random.choice(self.game.action_size, p=temperature_action_probs) # Divide temperature_action_probs with its sum in case of an error\n",
        "\n",
        "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
        "\n",
        "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
        "\n",
        "                if is_terminal:\n",
        "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
        "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "                        return_memory.append((\n",
        "                            self.game.get_encoded_state(hist_neutral_state),\n",
        "                            hist_action_probs,\n",
        "                            hist_outcome\n",
        "                        ))\n",
        "                    del spGames[i]\n",
        "                    \n",
        "            player = self.game.get_opponent(player)\n",
        "            \n",
        "        return return_memory\n",
        "                \n",
        "    def train(self, memory):\n",
        "        random.shuffle(memory)\n",
        "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
        "            state, policy_targets, value_targets = zip(*sample)\n",
        "            \n",
        "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
        "            \n",
        "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
        "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
        "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
        "            \n",
        "            out_policy, out_value = self.model(state)\n",
        "            \n",
        "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
        "            value_loss = F.mse_loss(out_value, value_targets)\n",
        "            loss = policy_loss + value_loss\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "    \n",
        "    def learn(self):\n",
        "        for iteration in range(self.args['num_iterations']):\n",
        "            memory = []\n",
        "            \n",
        "            self.model.eval()\n",
        "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
        "                memory += self.selfPlay()\n",
        "            \n",
        "            self.model.train()\n",
        "            for epoch in range(self.args['num_epochs']):\n",
        "                self.train(memory)\n",
        "            \n",
        "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPRRTJhMyuJz"
      },
      "outputs": [],
      "source": [
        "class SPG:\n",
        "    def __init__(self, game):\n",
        "        self.state = game.get_initial_state()\n",
        "        self.memory = []\n",
        "        self.root = None\n",
        "        self.node = None\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZIFIgBZsV_8"
      },
      "source": [
        "##EVALUATE-AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEPt8M7tsVVC"
      },
      "outputs": [],
      "source": [
        "class BigBrain(nn.Module):\n",
        "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        self.startBlock = nn.Sequential(\n",
        "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(num_hidden),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.backBone = nn.ModuleList(\n",
        "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
        "        )\n",
        "        \n",
        "        self.valueHead = nn.Sequential(\n",
        "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "        self.to(device)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.startBlock(x)\n",
        "        for resBlock in self.backBone:\n",
        "            x = resBlock(x)\n",
        "        value = self.valueHead(x)\n",
        "        return value\n",
        "        \n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, num_hidden):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
        "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        x += residual\n",
        "        x = F.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef7YiylitVP7"
      },
      "outputs": [],
      "source": [
        "class evaluateAI:\n",
        "    def __init__(self, model, optimizer, game, args):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        \n",
        "    def selfPlay(self):\n",
        "        memory = []\n",
        "        player = 1\n",
        "        state = self.game.get_initial_state()\n",
        "        \n",
        "        while True:\n",
        "            neutral_state = self.game.change_perspective(state, player)\n",
        "\n",
        "            action_probs = dict()\n",
        "            for action in self.game.get_valid_moves(state):\n",
        "                state_after_action = self.game.get_next_state(np.copy(state), action, player)\n",
        "                new_neutral_state = self.game.change_perspective(state_after_action, self.game.get_opponent(player))\n",
        "\n",
        "                value = self.model(\n",
        "                    torch.tensor(self.game.get_encoded_state(new_neutral_state), device=self.model.device).unsqueeze(0)\n",
        "                )\n",
        "                action_probs[action] = self.game.get_opponent_value(value) + 1\n",
        "            \n",
        "            memory.append((neutral_state, player))\n",
        "            action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n",
        "            state = self.game.get_next_state(state, action, player)\n",
        "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
        "            \n",
        "            if is_terminal:\n",
        "                returnMemory = []\n",
        "                for hist_neutral_state, hist_player in memory:\n",
        "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "                    returnMemory.append((\n",
        "                        self.game.get_encoded_state(hist_neutral_state),\n",
        "                        hist_outcome\n",
        "                    ))\n",
        "                return returnMemory\n",
        "            player = self.game.get_opponent(player)\n",
        "                \n",
        "    def train(self, memory):\n",
        "        random.shuffle(memory)\n",
        "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
        "            state, value_targets = zip(*sample)\n",
        "            \n",
        "            state, value_targets = np.array(state), np.array(value_targets).reshape(-1, 1)\n",
        "            \n",
        "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
        "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
        "            \n",
        "            out_value = self.model(state)\n",
        "            \n",
        "            value_loss = F.mse_loss(out_value, value_targets)\n",
        "            loss = value_loss\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "    \n",
        "    def learn(self):\n",
        "        for iteration in range(self.args['num_iterations']):\n",
        "            memory = []\n",
        "            \n",
        "            self.model.eval()\n",
        "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
        "                memory += self.selfPlay()\n",
        "                \n",
        "            self.model.train()\n",
        "            for epoch in range(self.args['num_epochs']):\n",
        "                self.train(memory)\n",
        "            \n",
        "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwFOCbM18f-2"
      },
      "source": [
        "##EVALUATE-AI-PARALLEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zipWWWZQ8jag"
      },
      "outputs": [],
      "source": [
        "class evaluateAIParallel:\n",
        "    def __init__(self, model, optimizer, game, args):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        \n",
        "    def selfPlay(self):\n",
        "        return_memory = []\n",
        "        player = 1\n",
        "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
        "        \n",
        "        while len(spGames) > 0:\n",
        "            states = np.stack([spg.state for spg in spGames])\n",
        "            neutral_states = self.game.change_perspective(states, player)\n",
        "            \n",
        "            for i in range(len(spGames))[::-1]:\n",
        "                spg = spGames[i]\n",
        "                \n",
        "                action_probs = dict()\n",
        "                for action in self.game.get_valid_moves(states[i]):\n",
        "                    state_after_action = self.game.get_next_state(np.copy(states[i]), action, player)\n",
        "                    new_neutral_state = self.game.change_perspective(state_after_action, self.game.get_opponent(player))\n",
        "\n",
        "                    value = self.model(\n",
        "                        torch.tensor(self.game.get_encoded_state(new_neutral_state), device=self.model.device).unsqueeze(0)\n",
        "                    )\n",
        "                    action_probs[action] = self.game.get_opponent_value(value.item()) / 2 + 0.5\n",
        "                \n",
        "                action_probs.update((key, value**self.args['temperature']) for key, value in action_probs.items())\n",
        "                spg.memory.append((neutral_states[i], player))\n",
        "                action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n",
        "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
        "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
        "                if is_terminal:\n",
        "                    for hist_neutral_state, hist_player in spg.memory:\n",
        "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "                        return_memory.append((\n",
        "                            self.game.get_encoded_state(hist_neutral_state),\n",
        "                            hist_outcome\n",
        "                        ))\n",
        "                    del spGames[i]\n",
        "                    \n",
        "            player = self.game.get_opponent(player)\n",
        "            \n",
        "        return return_memory\n",
        "                \n",
        "    def train(self, memory):\n",
        "        random.shuffle(memory)\n",
        "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
        "            state, value_targets = zip(*sample)\n",
        "            \n",
        "            state, value_targets = np.array(state), np.array(value_targets).reshape(-1, 1)\n",
        "            \n",
        "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
        "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
        "            \n",
        "            out_value = self.model(state)\n",
        "            \n",
        "            value_loss = F.mse_loss(out_value, value_targets)\n",
        "            loss = value_loss\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "    \n",
        "    def learn(self):\n",
        "        for iteration in range(self.args['num_iterations']):\n",
        "            memory = []\n",
        "            \n",
        "            self.model.eval()\n",
        "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
        "                new_memories = self.selfPlay()\n",
        "                memory += new_memories\n",
        "            \n",
        "            self.model.train()\n",
        "            for epoch in range(self.args['num_epochs']):\n",
        "                self.train(memory)\n",
        "            \n",
        "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll-vbL9tuN7e"
      },
      "source": [
        "##DUMMY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeGQH2FmcPaX"
      },
      "outputs": [],
      "source": [
        "class dummy():\n",
        "    def choose_action(self, game, game_state):\n",
        "        return random.choice(game.actions(game_state))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG7virNNvBrP"
      },
      "source": [
        "##ALPHA-BETA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CCW7ugWvJGa"
      },
      "outputs": [],
      "source": [
        "class alphabeta():\n",
        "    def choose_action(self, game, game_state):\n",
        "        player = game_state.player\n",
        "        value, move = self.max_value(game, game_state, player, -np.Inf, np.Inf)\n",
        "        return move\n",
        "\n",
        "    def max_value(self, game, game_state, player, alfa, beta):\n",
        "        if game.is_terminal(game_state):\n",
        "            return game.utility(player, game_state), None\n",
        "        value = -np.Inf\n",
        "\n",
        "        for action in game.actions(game_state):\n",
        "            value2, action2 = self.min_value(game, game.perform_action(action, game_state), player, alfa, beta)\n",
        "            if value2 > value:\n",
        "                value, move = value2, action\n",
        "                alfa = max(alfa, value)\n",
        "            if value >= beta:\n",
        "                return value, move\n",
        "        return value, move\n",
        "    \n",
        "    def min_value(self, game, game_state, player, alfa, beta):\n",
        "        if game.is_terminal(game_state):\n",
        "            return game.utility(player, game_state), None\n",
        "        value = np.Inf\n",
        "\n",
        "        for action in game.actions(game_state):\n",
        "            value2, action2 = self.max_value(game, game.perform_action(action, game_state), player, alfa, beta)\n",
        "            if value2 < value:\n",
        "                value, move = value2, action\n",
        "                beta = min(beta, value)\n",
        "            if value <= alfa:\n",
        "                return value, move\n",
        "        return value, move"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL64pm3PkkiD"
      },
      "source": [
        "##MCTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtllgetE5yGF"
      },
      "outputs": [],
      "source": [
        "class MCTSNode:\n",
        "    def __init__(self, game, args, state, parent = None, action_taken = None):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.action_taken = action_taken\n",
        "\n",
        "        self.children = []\n",
        "        self.expandable_moves = game.get_valid_moves(state)\n",
        "\n",
        "        self.visit_count = 0\n",
        "        self.value_sum = 0\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n",
        "    \n",
        "    def get_ucb(self, child):\n",
        "        q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
        "        return q_value + self.args['c'] * np.sqrt(np.log(self.visit_count) / child.visit_count)\n",
        "    \n",
        "    def select(self):\n",
        "        best_child = None\n",
        "        best_ucb = -np.inf\n",
        "        for child in self.children:\n",
        "            ucb = self.get_ucb(child)\n",
        "            if ucb > best_ucb:\n",
        "                best_child = child\n",
        "                best_ucb = ucb\n",
        "        return best_child\n",
        "    \n",
        "    def expand(self):\n",
        "        action = np.random.choice(np.where(self.expandable_moves == 1)[0])\n",
        "        self.expandable_moves[action] = 0\n",
        "        child_state = self.state.copy()\n",
        "        child_state = self.game.get_next_state(child_state, action, 1)\n",
        "        child_state = self.game.change_perspective(child_state, player = -1)\n",
        "\n",
        "        child = MCTSNode(self.game, self.args, child_state, self, action)\n",
        "        self.children.append(child)\n",
        "        return child\n",
        "\n",
        "    def simulate(self):\n",
        "        value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
        "        value = self.game.get_opponent_value(value)\n",
        "\n",
        "        if is_terminal:\n",
        "            return value\n",
        "        \n",
        "        rollout_state = self.state.copy()\n",
        "        rollout_player = 1\n",
        "        while True:\n",
        "            valid_moves = self.game.get_valid_moves(rollout_state)\n",
        "            action = np.random.choice(np.where(valid_moves == 1)[0])\n",
        "            rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
        "            value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n",
        "\n",
        "            if is_terminal:\n",
        "                if rollout_player == -1:\n",
        "                    value = self.game.get_opponent_value(value)\n",
        "                return value\n",
        "            rollout_player = self.game.get_opponent(rollout_player)\n",
        "    \n",
        "    def backpropagate(self, value):\n",
        "        self.value_sum += value\n",
        "        self.visit_count += 1\n",
        "        value = self.game.get_opponent_value(value)\n",
        "        if self.parent is not None:\n",
        "            self.parent.backpropagate(value)\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, game, args, model):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "    \n",
        "    def search(self, state):\n",
        "        root = MCTSNode(self.game, self.args, state)\n",
        "\n",
        "        for serach in range(self.args['num_searches']):\n",
        "            node = root\n",
        "\n",
        "            while node.is_fully_expanded():\n",
        "                node = node.select()\n",
        "            \n",
        "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "            value = self.game.get_opponent_value(value)\n",
        "\n",
        "            if not is_terminal:\n",
        "                node = node.expand()\n",
        "                value = node.simulate()\n",
        "            node.backpropagate(value)\n",
        "        \n",
        "        action_probabilities = np.zeros(self.game.action_size)\n",
        "        for child in root.children:\n",
        "            action_probabilities[child.action_taken] = child.visit_count\n",
        "        action_probabilities /= np.sum(action_probabilities)\n",
        "        return action_probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL_I0vnEk1Rx"
      },
      "source": [
        "#TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBkdJFzhN7fN"
      },
      "outputs": [],
      "source": [
        "class ELO_Tournament_Trainer:\n",
        "    def __init__(self, game, device, args, model, optimizer, number_of_players = 4, players = []):\n",
        "        self.game = game\n",
        "        self.number_of_players = number_of_players\n",
        "        self.players = players\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.args = args\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "    \n",
        "    def train_models(self, no_improvement = 10):\n",
        "        while len(self.players) < self.number_of_players:\n",
        "            player = evaluateAIParallel(copy.deepcopy(self.model), copy.deepcopy(self.optimizer), game, self.args)\n",
        "            player.learn()\n",
        "            self.players.append(copy.deepcopy(player))\n",
        "        \n",
        "        results = self.tournament()\n",
        "        \n",
        "        best_performer = max(results, key=results.get)\n",
        "        worst_performer = min(results, key=results.get)\n",
        "        print(f'best_performence: {max(results.values())}')\n",
        "        print(f'worst_performence: {min(results.values())}')\n",
        "        \"\"\"\n",
        "        self.players.remove(worst_performer)\n",
        "        best_performer = copy.deepcopy(best_performer)\n",
        "        best_performer.learn()\n",
        "        self.players.append(best_performer)\n",
        "        no_improvement -= 1\n",
        "        \"\"\"\n",
        "        return self.players\n",
        "    \n",
        "    def tournament(self):\n",
        "        players_performances = {player: 0 for player in self.players}\n",
        "        for player1 in self.players:\n",
        "            for player2 in self.players:\n",
        "                for _ in range(10):\n",
        "                    result = self.play_game(player1, player2)\n",
        "                    if result == 1:\n",
        "                        players_performances[player2] -= 1\n",
        "                    if result == -1:\n",
        "                        players_performances[player1] -= 1\n",
        "        return players_performances\n",
        "\n",
        "    def play_game(self, player1, player2, debug = False):\n",
        "        state = self.game.get_initial_state()\n",
        "        player = 1\n",
        "        while True:\n",
        "            if debug:\n",
        "                print(state)\n",
        "            action = self.choose_action(state, player, player1.model) if player == 1 else self.choose_action(state, player, player2.model)\n",
        "            state = self.game.get_next_state(state, action, player)  \n",
        "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
        "\n",
        "            if is_terminal:\n",
        "                if value == 1:\n",
        "                    return player\n",
        "                else:\n",
        "                    return 0\n",
        "            player = self.game.get_opponent(player)\n",
        "    \n",
        "    def choose_action(self, state, player, model):\n",
        "        neutral_state = self.game.change_perspective(state, player)\n",
        "\n",
        "        action_probs = dict()\n",
        "        for action in self.game.get_valid_moves(state):\n",
        "            state_after_action = self.game.get_next_state(np.copy(state), action, player)\n",
        "            new_neutral_state = self.game.change_perspective(state_after_action, self.game.get_opponent(player))\n",
        "\n",
        "            value = model(\n",
        "                torch.tensor(self.game.get_encoded_state(new_neutral_state), device=model.device).unsqueeze(0)\n",
        "            )\n",
        "            value = value.item()\n",
        "            action_probs[action] = self.game.get_opponent_value(value) / 2 + 0.5\n",
        "        \n",
        "        if self.args['temperature'] == 'inf':\n",
        "            return max(action_probs, key = action_probs.get)\n",
        "        \n",
        "        action_probs.update((key, value**self.args['temperature']) for key, value in action_probs.items())\n",
        "        return random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oewiz67-kKaA"
      },
      "source": [
        "##TIC-TAC-TOE-TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377,
          "referenced_widgets": [
            "2cf69e08b4f344dc9c4caf8cedfcf0b5",
            "3d70243d69ad4b28856e3415766eda96",
            "5659a3ef2b604d10b3b48c5c10142463",
            "2581ea1a497143cb9647052f33e78282",
            "67ba1450ede0436b9dc0991131a7b5b4",
            "16f49f485d934986a591968749876747",
            "66f4a4e4e4ab4085951937c9272ba824",
            "1379faa97599448c94c7767e22ff8829",
            "5235559ca2e54479a8e97fa7f4e5bebf",
            "7eddf86b08fb404781fdfc1ae918544c",
            "03f52c5bbabc45beb0f69b3bb2648544"
          ]
        },
        "id": "srtg_3Sd1KEr",
        "outputId": "f40d2124-3e1a-4832-812a-53f911837c6d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cf69e08b4f344dc9c4caf8cedfcf0b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-f7b1805caaf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mELO_Tournament_Trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-b50edc104696>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(self, no_improvement)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_players\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluateAIParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-4cbea9df7fd8>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mselfPlay_iteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_selfPlay_iterations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_parallel_games'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mnew_memories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselfPlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0mmemory\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_memories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-4cbea9df7fd8>\u001b[0m in \u001b[0;36mselfPlay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mnew_neutral_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_perspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_after_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_opponent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     value = self.model(\n\u001b[0m\u001b[1;32m     26\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoded_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_neutral_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-192cecbfef4f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mresBlock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackBone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalueHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-192cecbfef4f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "game = TicTacToe()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BigBrain(game, 8, 8, device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "args = {\n",
        "    'num_iterations': 1,\n",
        "    'num_selfPlay_iterations': 512,\n",
        "    'num_parallel_games': 128,\n",
        "    'num_epochs': 4,\n",
        "    'batch_size': 128,\n",
        "    'temperature': 1,\n",
        "    'dirichlet_epsilon': 0.25,\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "trainer = ELO_Tournament_Trainer(game, device, args, model, optimizer)\n",
        "players = trainer.train_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iItn3Yg626mk"
      },
      "outputs": [],
      "source": [
        "players"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BbNpeNbkRJL"
      },
      "source": [
        "##CONNECT-FOUR-TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Z3Kq3KkU0k"
      },
      "outputs": [],
      "source": [
        "game = ConnectFour()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BigBrain(game, 8, 8, device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "args = {\n",
        "    'num_iterations': 1,\n",
        "    'num_selfPlay_iterations': 512,\n",
        "    'num_parallel_games': 128,\n",
        "    'num_epochs': 4,\n",
        "    'batch_size': 128,\n",
        "    'temperature': 1,\n",
        "    'dirichlet_epsilon': 0.25,\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "trainer = ELO_Tournament_Trainer(game, device, args, model, optimizer)\n",
        "players = trainer.train_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZPgFsjiJ1EQ"
      },
      "source": [
        "#TESTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0La_wyg1qApl"
      },
      "source": [
        "##TIC-TAC-TOE-TESTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRG6Qtv0J4_U"
      },
      "outputs": [],
      "source": [
        "game = TicTacToe()\n",
        "win1 = np.array([[0, 0, 1], [-1, 1, 0], [1, -1, 0]])\n",
        "win2 = np.array([[1, 0, -1], [1, -1, 0], [1, 0, 0]])\n",
        "win3 = np.array([[0, 0, 0], [0, -1, -1], [1, 1, 1]])\n",
        "win4 = np.array([[1, 0, -1], [1, -1, 0], [0, 0, 0]])\n",
        "win5 = np.array([[0, 0, 1], [-1, 0, 0], [1, -1, 0]])\n",
        "wins = [win1, win2, win3, win4, win5]\n",
        "\n",
        "draw1 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
        "draw2 = np.array([[0, 0, 0], [0, -1, 0], [0, 0, 0]])\n",
        "draw3 = np.array([[-1, 1, -1], [-1, 1, -1], [1, -1, 1]])\n",
        "draws = [draw1, draw2, draw3]\n",
        "\n",
        "lose1 = np.array([[0, 0, -1], [1, -1, 0], [-1, 1, 0]])\n",
        "lose2 = np.array([[-1, 0, -1], [1, 0, 0], [-1, 1, 0]])\n",
        "lose3 = np.array([[-1, 0, 1], [-1, 0, 0], [-1, 1, 0]])\n",
        "loses = [lose1, lose2, lose3]\n",
        "\n",
        "for i, player in enumerate(players):\n",
        "    print(f'model{i}')\n",
        "    model = players[i].model\n",
        "    model.eval()\n",
        "    print('winning_possitions:')\n",
        "    for winning_position in wins:\n",
        "        encoded_state = game.get_encoded_state(winning_position)\n",
        "        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
        "\n",
        "        value = model(tensor_state)\n",
        "        value = value.item()\n",
        "        print(value)\n",
        "\n",
        "    print('\\ndrawing_possitions:')\n",
        "    for drawing_position in draws:\n",
        "        encoded_state = game.get_encoded_state(drawing_position)\n",
        "        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
        "\n",
        "        value = model(tensor_state)\n",
        "        value = value.item()\n",
        "        print(value)\n",
        "\n",
        "    print('\\nlosing_possitions:')\n",
        "    for losing_position in loses:\n",
        "        encoded_state = game.get_encoded_state(losing_position)\n",
        "        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
        "\n",
        "        value = model(tensor_state)\n",
        "        value = value.item()\n",
        "        print(value)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyhyUevTtFbh"
      },
      "source": [
        "#TOURNAMENTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FVITMsKtLEj"
      },
      "source": [
        "##TIC-TAC-TOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OonkC4p0p-xW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt6MmQY76_1x"
      },
      "source": [
        "#ADDITIONAL RESOURCES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztXYQ-NW7Brw"
      },
      "outputs": [],
      "source": [
        "class CRAZY_FAST_LEARNING:\n",
        "    def __init__(self, game, device, args, number_of_players = 4, players = []):\n",
        "        self.game = game\n",
        "        self.number_of_players = number_of_players\n",
        "        self.players = players\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.args = args\n",
        "    \n",
        "    def train_models(self, no_improvement = 10):\n",
        "        while len(self.players) < self.number_of_players:\n",
        "            model = BigBrain(game, 8, 8, device)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "            player = evaluateAIParallel(model, optimizer, game, self.args)\n",
        "            player.learn()\n",
        "            self.players.append(copy.deepcopy(player))\n",
        "        \n",
        "        while no_improvement > 9:\n",
        "            results = self.tournament()\n",
        "            \n",
        "            best_performer = max(results, key=results.get)\n",
        "            worst_performer = min(results, key=results.get)\n",
        "            print(f'best_performence: {max(results.values())}')\n",
        "            print(f'worst_performence: {min(results.values())}')\n",
        "            self.players.remove(worst_performer)\n",
        "            best_performer = copy.deepcopy(best_performer)\n",
        "            best_performer.learn()\n",
        "            self.players.append(best_performer)\n",
        "            no_improvement -= 1\n",
        "        return self.players\n",
        "    \n",
        "    def tournament(self):\n",
        "        players_performances = {player: 0 for player in self.players}\n",
        "        for player1 in self.players:\n",
        "            for player2 in self.players:\n",
        "                for _ in range(10):\n",
        "                    result = self.play_game(player1, player2)\n",
        "                    if result == 1:\n",
        "                        players_performances[player2] -= 1\n",
        "                    if result == -1:\n",
        "                        players_performances[player1] -= 1\n",
        "        return players_performances\n",
        "\n",
        "    def play_game(self, player1, player2):\n",
        "        state = self.game.get_initial_state()\n",
        "        player = 1\n",
        "        while True:\n",
        "            action = self.choose_action(state, player, player1.model) if player == 1 else self.choose_action(state, player, player2.model)\n",
        "            state = self.game.get_next_state(state, action, player)  \n",
        "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
        "\n",
        "            if is_terminal:\n",
        "                if value == 1:\n",
        "                    return player\n",
        "                else:\n",
        "                    return 0\n",
        "            player = self.game.get_opponent(player)\n",
        "    \n",
        "    def choose_action(self, state, player, model):\n",
        "        neutral_state = self.game.change_perspective(state, player)\n",
        "\n",
        "        action_probs = dict()\n",
        "        for action in self.game.get_valid_moves(state):\n",
        "            state_after_action = self.game.get_next_state(np.copy(state), action, player)\n",
        "            new_neutral_state = self.game.change_perspective(state_after_action, self.game.get_opponent(player))\n",
        "            \n",
        "            value = model(\n",
        "                torch.tensor(self.game.get_encoded_state(new_neutral_state), device=model.device).unsqueeze(0)\n",
        "            )\n",
        "            value = value.item()\n",
        "            action_probs[action] = self.game.get_opponent_value(value) / 2 + 0.5\n",
        "        \n",
        "        if self.args['temperature'] == 'inf':\n",
        "            return max(action_probs, key = action_probs.get)\n",
        "        \n",
        "        action_probs.update((key, value**self.args['temperature']) for key, value in action_probs.items())\n",
        "        return random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iww9T60s7KTt"
      },
      "outputs": [],
      "source": [
        "game = TicTacToe()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "args = {\n",
        "    'num_iterations': 1,\n",
        "    'num_selfPlay_iterations': 1024,\n",
        "    'num_parallel_games': 128,\n",
        "    'num_epochs': 4,\n",
        "    'batch_size': 128,\n",
        "    'temperature': 1,\n",
        "    'dirichlet_epsilon': 0.25,\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "trainer = CRAZY_FAST_LEARNING(game, device, args)\n",
        "players = trainer.train_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONbuTYjb7M7D"
      },
      "outputs": [],
      "source": [
        "players"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4ww-ndn7OwY"
      },
      "outputs": [],
      "source": [
        "game = TicTacToe()\n",
        "win1 = np.array([[0, 0, 1], [-1, 1, 0], [1, -1, 0]])\n",
        "win2 = np.array([[1, 0, -1], [1, -1, 0], [1, 0, 0]])\n",
        "win3 = np.array([[0, 0, 0], [0, -1, -1], [1, 1, 1]])\n",
        "win4 = np.array([[1, 0, -1], [1, -1, 0], [0, 0, 0]])\n",
        "win5 = np.array([[0, 0, 1], [-1, 0, 0], [1, -1, 0]])\n",
        "wins = [win1, win2, win3, win4, win5]\n",
        "\n",
        "draw1 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
        "draw2 = np.array([[0, 0, 0], [0, -1, 0], [0, 0, 0]])\n",
        "draw3 = np.array([[-1, 1, -1], [-1, 1, -1], [1, -1, 1]])\n",
        "draws = [draw1, draw2, draw3]\n",
        "\n",
        "lose1 = np.array([[0, 0, -1], [1, -1, 0], [-1, 1, 0]])\n",
        "lose2 = np.array([[-1, 0, -1], [1, 0, 0], [-1, 1, 0]])\n",
        "lose3 = np.array([[-1, 0, 1], [-1, 0, 0], [-1, 1, 0]])\n",
        "loses = [lose1, lose2, lose3]\n",
        "\n",
        "for i, player in enumerate(players):\n",
        "    print(f'model{i}')\n",
        "    model = players[1].model\n",
        "    model.eval()\n",
        "    print('winning_possitions:')\n",
        "    for winning_position in wins:\n",
        "        encoded_state = game.get_encoded_state(winning_position)\n",
        "        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
        "\n",
        "        value = model(tensor_state)\n",
        "        value = value.item()\n",
        "        print(value)\n",
        "\n",
        "    print('\\ndrawing_possitions:')\n",
        "    for drawing_position in draws:\n",
        "        encoded_state = game.get_encoded_state(drawing_position)\n",
        "        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
        "\n",
        "        value = model(tensor_state)\n",
        "        value = value.item()\n",
        "        print(value)\n",
        "\n",
        "    print('\\nlosing_possitions:')\n",
        "    for losing_position in loses:\n",
        "        encoded_state = game.get_encoded_state(losing_position)\n",
        "        tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
        "\n",
        "        value = model(tensor_state)\n",
        "        value = value.item()\n",
        "        print(value)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03f52c5bbabc45beb0f69b3bb2648544": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1379faa97599448c94c7767e22ff8829": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16f49f485d934986a591968749876747": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2581ea1a497143cb9647052f33e78282": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eddf86b08fb404781fdfc1ae918544c",
            "placeholder": "​",
            "style": "IPY_MODEL_03f52c5bbabc45beb0f69b3bb2648544",
            "value": " 0/4 [00:06&lt;?, ?it/s]"
          }
        },
        "2cf69e08b4f344dc9c4caf8cedfcf0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d70243d69ad4b28856e3415766eda96",
              "IPY_MODEL_5659a3ef2b604d10b3b48c5c10142463",
              "IPY_MODEL_2581ea1a497143cb9647052f33e78282"
            ],
            "layout": "IPY_MODEL_67ba1450ede0436b9dc0991131a7b5b4"
          }
        },
        "3d70243d69ad4b28856e3415766eda96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16f49f485d934986a591968749876747",
            "placeholder": "​",
            "style": "IPY_MODEL_66f4a4e4e4ab4085951937c9272ba824",
            "value": "  0%"
          }
        },
        "5235559ca2e54479a8e97fa7f4e5bebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5659a3ef2b604d10b3b48c5c10142463": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1379faa97599448c94c7767e22ff8829",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5235559ca2e54479a8e97fa7f4e5bebf",
            "value": 0
          }
        },
        "66f4a4e4e4ab4085951937c9272ba824": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67ba1450ede0436b9dc0991131a7b5b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eddf86b08fb404781fdfc1ae918544c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
